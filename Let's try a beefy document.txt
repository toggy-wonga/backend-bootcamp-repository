Interlude
The Shock
Excerpt from “The Blood of the Earth: The Economic Downfall of the Pre-Great Death Societies” by Alaric O’Braunfels, Munich Press, 972 ASJ


Photograph of an abandoned oil well located in North Dakota, 1997

https://youtu.be/KjWd5WetF8c?si=wE6KnsMPQC88XgLX 

After the Sinai War of 1988, over 90% of the world’s crude oil supplies went up in flames. Fires burned indiscriminately for almost a century, and now, much of the former Middle East and parts of North Africa are vast fields of grey-green glass, mountains of ash and scorching sands. Very few live in that part of the world now, but in the areas that remain, huge treasure troves of information from this era have been uncovered. 

Many countries in the world were not so concerned about the massive loss of life as they were for the monumental loss of oil. The dirt-cheap oil from Iran, plus the Three Mile Island incident and the subsequent lobbying by Big Oil, had made almost every single country entirely dependent on oil not only for their infrastructure, but their economy as well. Oil-based products were ubiquitous - just about everything was made from oil in some way, be it the rubber and synthetic fibers made for clothing, electronic insulation materials, disposable plastic bags for food, and even medicines and fertilizers were made in processes utilizing petroleum. 

And so, it was a massive shock to the rest of the world when suddenly the price of everything jumped. Medical equipment inflated by almost 2,500%, and kept climbing. Many pharmacies went out of business by 1995. Rubber used for manufacturing cars nosedived, and the last tires were made by 1991, with everything afterwards made using recycled rubber. Rubber was later collected and hoarded by the remaining industries after 2000, as it was needed for making hoses and gaskets for valuable machinery such as filtration systems and pumps. Electronics miniaturization was permanently reversed, as with the exponential increase to electricity bills, industries no longer prioritized making objects such as computers and monitors smaller and smaller. Instead of using plastics to insulate their products, they now had to use heavy, bulky ceramics and metals. These materials also would hold on to heat for longer periods of time, requiring computers and other essential electronics to become larger and larger. It also, unfortunately, made them more dangerous, as chipped ceramics and heated metal caused industrial accidents to slowly but gradually increase. Hundreds of thousands of companies went bankrupt between 1988-1992, and entire industries went completely extinct, never to be revived again. Telecommunications are only just now making a comeback, in the 30th century. 


Train yard outside of Minneapolis, Minnesota, 1990. Many of these trains would never run again. 

The first area to be affected by this massive drop in fuel was of course the transportation network that had connected the world during the Glorious Decades. When an American bought a pack of cigarettes from a store, they were buying tobacco from Brazil, paper made from trees in Zaire, placed in an embossed plastic box manufactured in China, and shipped using Iranian oil across the United States to be bought at a gas station in Olympia, Washington. 

By the end of the summer of 1988, not even a year after the Sinai War, the Crash and the Shock, global trade had practically vanished. Many countries were busy trying to keep their essential industries and commuter networks running, and had no resources for international shipping and exporting. Shipping all but dried up by 2000, and until then they carried only the most essential supplies such as crude oil, food imports, fertilizers and raw metals. By 2010, many cargo ships had been disassembled for parts, or were turned into housing for the poor, like in New York City. 

Personal automobiles had fallen out of favor within a few years of the Crash, and instead, many governments opted for mass public transportation programs. Oil and later coal powered buses dominated the streets of American metropolises until the late 2000s, while in places like the European Federation, Nordic Union, UK and Japan saw electric powered trains and rail lines. The UKs were shut down in late 2009, while the Federation shut down theirs in 2014, citing “increased terrorist activity”. Japan’s public transportation system held on the longest, as it was only shut down in 2024 during the Second Great Heat Wave. In other countries, like the USSR, China, and Patagonia, many people lived close enough to where they worked that commuting wasn’t an issue, and in the rare cases it was, horse-drawn carriages and rickshaws became the dominant method of transporting workers to and from their homes. 

Planes remained in use, though only for the very, very rich. Cargo planes, mainly running on a mixture of dirty liquid coal and natural gas, frequently transported important plutocrats across the USA and European Federation, respectively. In other places, such as Australia and the USSR, blimps had actually begun to see a comeback and replaced the conventional aircraft of their militaries - by 2027, only the USSR, Australia and New Zealand, Japan and the Patagonian Alliance had functional air forces. 


Two women in the Great American Dust Bowl in 2024, wearing clothing made in the early 1990s

Clothing was the most visible casualty of this massive drop of oil supplies. Much of fashion, from not just the west but around the world, relied on synthetic dyes and oil-based looms. A lack of reliable fertilizers, and worldwide famines, led to most textile plants such as cotton, linen, and hemp (which was still illegal, but this hadn’t been seriously enforced since 1989) to see reduced yields as arable land was prioritized for growing food for the now stagnant and decaying metropolises. Of course, for the very rich, they could still buy new synthetic clothing and silks, but the most expensive thing was dyes. In the 1990s, having obnoxiously colored clothing was a sign of wealth, as opposed to the Glorious Decades, when it was considered tacky to wear such obnoxious clothing. Instead, many in the 1970s and 1980s in the west wore suits and dresses made from silk and leather, while only the very poor were forced to wear these synthetic clothes made with bright, neon colors. 

This led to a massive shift in how clothing was viewed and worn. Previously, anyone could buy a new set of tuxedos for about the same price as a cheeseburger, and when a hole showed up, they’d simply buy a new one. However, after the Crash, new clothing became frighteningly expensive. Materials, both synthetic and natural, dwindled in supply and soon, everyone was thrifting. This led to a period from 1988-1991 when everyone was wearing second-hand clothing from a variety of different fashion periods. Some men even chose to wear dresses. In the 1990s, many people only bought one or two new clothes a year, usually for special occasions like weddings, graduations, or other big celebrations. But by the 2000s, many were too poor to afford new clothes at all. The Smith Administration introduced a minor program in 2003 that focused on this, as the civil unrest in the South had significantly reduced the supply of cotton and other textiles. With this program, called the American Clothing Act of 2003, granted all Americans one free set of clothing a year, and gave them up to five coupons for repairs that could be exchanged at any clothing store. This program showed some promise, but after the Kentucky-based Bowling Green Textiles Company bought up all the working textile mills, Government-Issue clothing became far more drab. This led to a brief period between 2007-2014 where clothing was made from a patchwork of different styles, textiles, and colors, referred to as “rainbow style”. Severe wear-and-tear from the elements, pollution and from being handed down to so many people led to many of these patchwork clothes to break down by the mid 2010s. On top of that, the supplies used to repair textiles were in very short supply by this point, and BGTC had monopolized the remaining supply shops. In 2015, they changed their policy so that only BGTC-manufactured clothes would be repaired at their shops, and only then if they hadn’t been modified by the owners. With the loss of dyes, this led to a dramatic decrease in variety and color for clothing, with many people now wearing simple, undyed cotton clothing that eventually became eternally stained a dull brown through years of exposure to sweat, acid rain, and pollution. 

Other countries had seen similar changes in fashion, though some of them weren’t all a result of supply shortages. In the Soviet Union, for example, all citizens were required to wear denim jackets and pants, dyed red with beets, while soldiers and police were required to wear dress uniforms when not on active duty. In Europe, people wore light shirts and shorts made from linen, to help keep their bodies cool. In much of Latin America and Africa, the death rate was high enough that there was a steady supply of second-hand clothing, but when new clothing was made, it was usually made from hemp, as many cartels grew it on the side. One exception is in the Patagonian Alliance, where many citizens wore austere but comfortable wool clothing made from Argentine goats, alpacas and llamas. In Australia and New Zealand, their main textile was actually a mix of leather and synthetic fibers from recycled materials from the Glorious Decades - as their populations were extremely small, they could afford to make and reuse the old materials from that time. In China, however, they used whatever they could find - cotton, hemp, linen, and as some reports implied, human hair. Clothing was always in short supply in China - it was not uncommon to see children as young as 10 running around naked, never having worn clothing in their life. Japan went through a similar fashion cycle as the USA, though they settled on robes and kimonos evocative of feudal Japan. 


Food, Inc Corn Farm #5634-6745N, located outside of North Platte, Nebraska. Photographed 2023

Another industry that was completely crippled by the death spiral of oil supplies was the agricultural industry. In many poorer countries in Africa, Latin America and South Asia, farming systems broke down by 1991 as the heat and humidity caused many tractors, mills, and other farming equipment to stop working. Because so many of these machines were entirely dependent on oil, they also stopped working throughout the 1990s. This forced much of the world to tend to their crops by hand - a time-consuming process that suffered the risk of heat stroke. Other, richer countries, like the Big Four, were able to keep their agricultural equipment running into the late 2000s, with research conducted into making steam and coal powered farming equipment. These jury-rigged tractors and combine harvesters carried the risk of exploding and setting the whole field on fire, and also decreased the yield of the harvest by contaminating the ground. 

But another major problem arose by the turning of the second millennium - the dramatic decrease in fertilizer production. In order to meet demand in the ever-growing cities, without fertilizers to reinvigorate the ground, many farms had to make bigger and bigger fields crammed with crops. In the 1990s, vermin such as moles, gophers, and rats were drawn to the plants, and insecticides were too expensive to produce. Some farms began building walls around their fields, and by 1995, many employed armed guards to routinely hunt for rodents. After the Great American Famine of 2000, these guards now also protected the crops from thieves and starving homeless. Without the fertilizers, the farmers had to work the ground harder and harder every year, until in the late 90s, a massive dust bowl started to form in Oklahoma, climbing all the way up to Montana by 2027. 

Industrial fertilizers were still able to be produced, of course, just in an incredibly small amount. The farms that initially bought all these fertilizers were the ones owned by Food, Inc, and because they were able to produce more food (and thus sell more), they put the other farms out of business. By 2004, Food, Inc had an effective monopoly on the synthetic fertilizer supply in the United States, but even then, other farms got by using older, more tried and true methods such as using manure. Septic waste was pumped in from cities, and while initially it showed promise, the numerous chemicals and contamination in the waste proved too toxic to the plants, causing entire fields to die out. Eventually, a method was developed to separate the human waste from the chemicals, but even then, it was extremely costly. By 2010, fertilizers were considered too costly to produce, with every country (except for Canada, the USSR, and New Zealand) switching to manure-based crop fertilization. 


Hospital in Leeds, UK. Photographed 2013

Medical technology and the production of pharmaceuticals also dramatically declined after the Shock. Disposable medicines went extinct by 1993, and many medicines such as painkillers, anesthesia and antibiotics became very costly to produce, leading to alcohol to be the most popular way of dulling pain. The decline in antibiotics also contributed to the spread of many diseases around the world. By 2005, many pharmaceutical companies had shut down, with the functional ones falling under the umbrella of Bethesda PharmTech Inc. The Sweeney Administration was noted for shifting resources away from medical and pharmaceutical production into agricultural and industrial production. Medical supplies also heavily relied on plastics, so things such as IV tubes, syringes and other tools became harder and harder to produce, which forced the medical industry to recycle them. This led to sterilizing agents to be used up faster and faster, with alcohol replacing it. It was speculated that at least three outbreaks of Hepatitis C in 1997 alone resulted from the use of improperly sanitized IV drips.  

As a combination of reduced energy supplies, the difficulty of replacing parts, and electrical malfunctions, freezers across the world became more and more costly to keep running. Organ transplant and blood transfusions became almost impossible to perform, unless the donors were local, ready and willing. The lack of reliable refrigeration also heavily affected the food supply, as places like supermarkets and warehouses rotted during brownouts. People eating improperly refrigerated food caused gastrointestinal diseases to skyrocket and yet more medical resources to be used. Across the world, refrigerators became centralized and standardized, with working refrigerators seized by the government in Europe, North America and Asia. Many countries, by 2000, had opted to use mainly non-perishible rations made out of their staple food, like Ration Beta in the USA, Ful-Tec Natural in the EF, and the ubiquitous tortillas made all across Latin America. 

Like medical supplies, food became harder and harder to package, as disposable plastic-based containers and wrappers quickly became impractical. Paper bags instead became the most common way of transporting food and medicine, with glass bottles for liquids and some food occasionally being stored in metal cans, though metal was usually prioritized for industrial repairs and military equipment. 

Most furniture also saw a massive decrease in production, with wood being prioritized for burning, and textiles being prioritized for clothing rather than cushions. Some plastic furniture was melted down in the mid 1990s in an attempt to recycle them, but nothing came of it other than extremely toxic fumes. Many apartments became austere and barren, with most people across the world simply having a bed frame, a table, one or two dressers, and two to three dirty, rickety chairs. A large industry sprung up in the USA revolving around repairing and maintaining furniture made before the Glorious Decades, and unlike the clothing industry, this one was never monopolized and thus never died out. For the extremely wealthy, it became common for people to use furniture exclusively made out of glass that was dyed in random colors - glass was much easier to produce than wood and metal. However, things such as chairs quickly fell out of favor as they tended to shatter rather violently. In China, furniture was usually made with metal, or with wicker if you were a high-ranking member of the CCP. The USSR adopted several Kazakh customs, such as sitting on the ground instead of a chair when eating, to reduce the amount of furniture produced so that wood and metal could be focused on things like buildings and weapons. 

The electronics industry was almost wiped out by the Shock. Disposable electronics became almost impossible to manufacture, as almost no country had enough oil to spare for the plastics or enough metal to make the circuitry. Most computer technology regressed, landing at a roughly 1940s-era level by 2027. Computers were big and bulky, filled with exposed wires, circuitry, and prone to overheating and exploding. CRT screens became prohibitively expensive, with a general switch to colored buttons. In the 2020s, many countries began sifting through landfills and junkyards for valuable bits of electronics from the Glorious Decades, mainly containing traces of silver, copper, gold and aluminium. By the Solar Flare, many countries had considered advanced electronic devices to be wastes of energy and resources, and as such, many electronic devices were kept as primitive (and practical) as possible, consisting of glorified on/off switches as opposed to computers. Most of the elites in the Big Four, Patagonia, Australia and New Zealand, and Japan had access to Glorious-Decades level technology, complete with LCD screens, neon tubes and hard disk drives. 

The lack of synthetic rubber also strangled the life out of many industries. Hoses, gaskets, tubes and insulators all became impractical to produce, let alone tires. A vast majority of industrial accidents between 1988-2027 are believed to have been caused by improperly maintained rubber components. In places like China and the USA, there was a drive to switch to pure metal-based tubes but that had various degrees of success and failure. The rubber components in machines made them particularly vulnerable to sabotage, a trait that the AVA and Pilgrims exploited heavily. 

So, how did the world deal with this massive drop in fossil fuels? Well, for the most part, they had to use more fossil fuels. Energy grids shifted from being oil-based to coal and natural gas based. Most supplies of oil ran out around 2005, though a few places in Australia and Canada had left over reserves all the way into 2027. Some countries used more natural gas than coal, but around 2015, most accessible deposits of natural gas had been exhausted. It was known that this would happen for some time now, and all the way back in the early 1990s, countries had began experimenting with various ways to bridge the gap and replace their pollution-spewing coal-based infrastructure. 


The Banfora Solar Farm, Greater Senegal. Taken on June 29th, 965 ASJ

A lot of the alternative energies used today, in the 30th century, were developed in Australia and New Zealand and Japan. The “Green Quartet” (nuclear, solar, wind and hydroelectric power) had their origins in those two countries, but were not able to be produced on a large scale until the Xhosan Solar Renaissance of 435 ASJ (2462 AD). 

Japan, long reliant on imports of coal and oil, saw their infrastructure rapidly decay over the course of the 1990s. An initiative to switch from personal automobiles to public transportation helped things, and energy was rationed so strictly that entire apartment blocks became bathed in darkness for days at a time. Before the Emperor’s suicide in 2006, and the “Great Departing”, the keenest minds in Japan had come together to make the most of their extremely limited resources and develop alternate ways of fueling Japan’s vehicles, infrastructure and supply lines. Of these, hydroelectricity and nuclear were for the most part, already in use - Japan had a few nuclear power plants that had been safely shut down in 1980, and all of them were up and running again by the year 2000. Japan had only 12 nuclear power plants before the TMI incident, and while they did not shut them down, they were deliberately kept on levels to safely minimize their output. The Japanese Nuclear Safety Commission heavily regulated the gradual ramping up of the reactors, but demand seemed to only increase despite the austerity measures in place by the government. In 2003, rolling blackouts caused a few of the overwhelmed and understaffed plants to be evacuated out of fears that they could explode similar to TMI. The first major nuclear plant after 1988 was built in 2004, outside of Tokyo. 

As a result of the aftermath of the 2003 heat waves, Japan’s Minister of Energy commissioned research into geothermal energy sources. Japan had actually dabbled with geothermal power stations all the way back in the 1920s, and so several prototype stations were designed. They were built in isolated rural zones (as rural as one could get in Japan), and away from population centers, and used active volcanos to heat water and used the steam to turn turbines, thus generating electricity. Ocean water was used, and the first major prototype geothermal power plant, located in Yanaizu-Nishiyama, was opened in 2005 and was capable of outputting 75 MWe. 

Geothermal was costly to create for how little energy they produced, and Japan was very careful to expand their nuclear power, as their supplies of uranium were extremely limited, though Japan was in the process of filtering radioactive particles from sea water, mixing it with filler, and creating “dirty rods” for reactors. Solar power was also considered, however the skies of Japan were far too cloudy for it to be practical. 

Hydroelectricity remained a large producer of electricity, with the Minister of Energy working with the Minister of Agriculture, Forestry and Fisheries to implement hydroelectric dams into the irrigation systems of Japan. In 2004, plans were drafted up to create a major hydroelectric dam outside of Tokyo to power a series of desalination plants, but construction did not start before the suicides. 

Almost all of Japan’s research was lost in the “Great Departing”, as numerous scientists, disillusioned with life, leaped off of buildings. Many nuclear power plants almost went critical, as depressed workers simply didn't show up to work. Japan’s Minister of Energy, Takumi Aoyama, struggled to form a cohesive work force with the will to live to keep the country functioning. He would end up founding the Steward Society. 

It would take a full 11 years later for Japan to rebuild itself enough to once again focus on expanding its power sources. By this time, in 2017, Japan’s energy infrastructure had been entirely dedicated to desalination and food processing, with the last of Japan’s coal, oil and natural gas used up. It was here that Japan made contact with the Union of Australia and New Zealand, and began to import not just uranium, concrete, and other materials for nuclear power plants, but experimental thin-film solar cells as well. Solar batteries were first prototyped by New Zealand in 1990, while they were advanced and upgraded by Australia in the late 2000s. It wasn’t until ANZ exchanged technology with Japan that the solar cell was finally perfected. 

Dr. Martin Kea Rutherford is credited as developing the thin-film photovoltaic solar cell, for the New Zealand Minister for Energy. The cells were initially time-consuming to produce, with the first factory producing them once every 34 hours. It was after exchanging technology with Australia that the process was significantly sped up, with efficiency peaking at just 86 minutes per cell in 2007. Australia later developed bifacial cells, allowing both sides of the panel to receive power, though this increased production time to 2 hours 42 minutes. Australia, with its vast, open deserts, proved to be the more efficient country to build solar farms in, and factories across Australia’s east coast churned out panel after panel until they ran out of silicon in 2025. 

When the Steward Society got ahold of the first shipment of ANZ solar cells, the Prefect for Technological Development created a program at the First National Academy for Technological Education devoted entirely to increasing the efficiency of the solar cells. While a lot of the smog had cleared from the skies of Japan, it was still overcast more often than not, and the production output of the solar cells was severely limited (as they had been designed to be used in crystal-clear skies above the Australian Outback). It was in 2020 that Japan was finally successful in their endeavor, as scientists at the FNATE developed the first multi-junction solar cell, allowing it to receive power from different wavelengths of light, significantly increasing the overall effectiveness of the cell, and allowing (albeit minimal) electrical collection during cloudy, overcast days and during dawn and dusk. 

These solar cells then spread all throughout Japan and the Union of Australia and New Zealand. Japan’s decaying public transportation system (just a series of trains in the largest 4 cities in the country) became entirely solar powered, though the trains were finally shut down in 2024 as the solar panels literally melted in the heat. 


Offshore Wind Farm located off the coast of Auckland, Old Zealand. Photo taken around 341 ASJ.

Another technology perfected by ANZ and Japan was wind power. Wind turbines were built along New Zealand's coast in the 1990s, but technology remained primitive and inefficient until the early 2020s. By the 2020s, New Zealand was powered by armies of solar panels, wind farms and even a few geothermal substations, in addition to a hydroelectric integrated irrigation system. The scientists in the Steward Society, upon seeing the schematic, proposed a number of changes that dramatically increased the stability of the structures, allowing for larger offshore turbines to be built. 

Australia and New Zealand and Japan were exceptions though - much of the world continued to limp along on oil, then natural gas, then coal, becoming dirtier every year. In places like Africa, Latin America, and the former middle east, many simply burned whatever was on hand to create steam to power ancient, Victorian-era machinery like boilers and steam engines. A few countries in these regions, such as Brazil and Mexico, experimented in hydroelectricity and solar power, respectively, but these projects never left the prototyping stage due to the political instability plaguing their nations. 

Nuclear power plants were built across many countries, however these “dirty reactors” were, for the most part, a rushed construction more focused on supplying power than the safety of the workers inside them or those living nearby. The only countries that managed to perfect nuclear energy were the USSR and Australia, with the USSR (after many, many attempts) developing nuclear powered train engines by 2010, though they were incredibly crude and bulky. These advanced nuclear power plants eventually migrated to the Nordic Union, Poland, and East Germany, though they remained large and bulky, incapable of outputting a fraction of the MWe that a Soviet, Australian or Patagonian reactor. Recently uncovered evidence found during the Sinai Expedition of 817 ASJ has pointed to the ancient nation of Israel as extensively utilizing nuclear power for their Kibbutz bunkers. Israel’s nuclear power plants were in the middle - they weren’t as advanced as those in Europe or Patagonia, but they weren’t as crude as the experimental ones built in North America and Australia. Of the 100+ Israeli bunkers discovered, more than half of them have become saturated with radiation, now pristine portals into the past much like the ancient ruins of Pompeii once were. 

In many cases, once a country had run out of oil, they switched to coal. Ironically, in the countries most vulnerable to this (mainly in Africa) their transportation and infrastructure weren't entirely reliant on petroleum and oil, so their supply lines remained relatively intact, but severe famines and resource shortages devastated entire economies. A few ancient African electrical infrastructures remain intact, if non-functional, giving archaeologists a look into the engineering of the past. 


The Hoof-Er Dam, completed sometime before the Black Ages (many historians agree around 1936). Destroyed in 254 ASJ in a battle between the New Cascadian Republic and the Legion of Canaan. 

Hydropower was a double edged sword in many cases - dams could provide ludicrous amounts of energy, with the ancient Guri Dam (completed in 1978) in what was once Venezuela producing over 10,000 MWe. However, chronic drought cycles and irrigation demands usually forced hydroelectric dams to be decommissioned. Other times, governments believed that the energy supplied from them outweighed the fact that a few thousand people would starve to death. Ancient China is known for building a vast majority of such dams, including the Three Gorges and Ertan Dams, during the early 2000s. This caused already strained water supplies to be diverted from their normal water cycle, and is thought to be one of the contributors to the mass die-off of vegetation in mainland China in the Black Ages. 

The European Federation was also known for over-utilizing hydroelectric dams in the 1990s, leading to mass famines and drought cycles to persist well into the 22nd century. A majority of their dams were also built in the now flooded region known as the “Low Countries”, leading to 90% of their hydroelectric power stations to be underwater by the 2020s. The European Federation was initially a leader in alternate energies in the immediate aftermath of the Shock, as numerous prototype solar farms, wind farms, hydroelectric dams and nuclear power plants were built along the former regions of Iberia, France, and Italy. The European people were quite rowdy though, used to the instant gratification of the Glorious Decades, and did not take the policy of austerity well. Far-left and Far-right terrorism hampered the development of many solar farms, and a nuclear accident outside of Brussels in 2011 led to all of their nuclear power plants to be decommissioned. Like the UK, the coal miners and their unions forced many governments to stay on the path of fossil fuels, with coal becoming the dominant source of power in the EF and UK by 2015. 

The only exception to this was the state of Denmark (now part of the Kingdom of Norway), as their wind and hydroelectric farms were not only efficient, but were able to be used on ocean water instead of freshwater rivers usually reserved for irrigation. Because of this, wind and hydroelectric power were allowed to thrive and evolve over time, with the UK, Ireland, and Nordic Union all receiving prototype plants during their occupations by Brussels. In the 2010s, these technologies were replicated around Europe, and now the Irish Isles are powered by vast, off-shore wind farms. The Nordic Union leaned a bit more towards nuclear power though, however it was merely a shield to develop nuclear weapons that they could use to deter the Lion of Brussels and the Bear of Moscow from invading them again. 

East Germany (now the Holy Empire of Europa) managed to integrate tiny hydroelectric dams into their irrigation canals and vertical step farms, much like how Japan and ANZ integrated into their farms and cities, though East Germany’s were far less efficient and generated only a fraction of their Pacific counterparts. Still, East Germany is notable as the only country in Europe to notably integrate hydroelectric power into their energy grid and not result in major droughts due to the mismanagement of rivers and lakes. 

Poland on the other hand, much like its history in the Black Ages, was a mixed bag. They used a little bit of everything. Mining unions kept coal in circulation, while small-scale hydroelectric dams caused drought, but not enough to be a concern to Kowalski. Stolen nuclear research from the Soviets allowed for some limited production, and a few wind farms were built in fields too polluted to grow crops in. 

The United States ended up being hit the hardest when it came to trying to dig their way out of reliance on fossil fuels. Hydroelectric dams in the Southwest contributed to the rapidly expanding desert, while geothermal power proved to be too inefficient to supply America’s gargantuan energy needs. The skies above America remained eternally smoggy, so solar was out of the question. Corn-based ethanol was prototyped in the early 2000s, but the Smith Administration deemed it more important to use that corn to feed the growing number of starving unemployed. Some nuclear power plants were recommissioned in the northwest (now the Cascadian Confederacy in the present day), and a few dams were built along the Great Lakes, but America greedily gobbled up coal like their livelihood depended upon it - which, for the most part, it did. 


Nuclear power plant located in New Tokyo (formerly Yelizovo), Japan

In the present day, fossil fuels are very, very rarely used, mainly only a novelty or by researchers studying pre-Great Death technology. Boiling polluted water for environmental purification is a large source of energy in the reconstruction and geoengineering programs in Africa and Latin America. Research into heat-resistant solar farms are being conducted, with a Xhosan company hoping to turn the entire Middle East into a massive solar farm by the end of the 30th Century. 

